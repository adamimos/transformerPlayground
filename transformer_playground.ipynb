{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A small DIY transformer\n",
    "\n",
    "by Adam Shai\n",
    "2023.01.21\n",
    "\n",
    "following Karpathy's youtube video:  https://www.youtube.com/watch?v=kCc8FmEb1nY\n",
    "shakespear text data is called `shakespear_data.txt`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Shakespear Data\n",
    "this is the \"tiny shakeseapr\" dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length of the data is 1115393 characters\n"
     ]
    }
   ],
   "source": [
    "# open the shakespear_data.txt file and read in\n",
    "with open(\"shakespear_data.txt\") as f:\n",
    "    data = f.read()\n",
    "\n",
    "# print the length of the data\n",
    "print(\"the length of the data is {} characters\".format(len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the first 250 characters in the data\n",
    "print(data[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length of the vocabulary is 65 characters\n"
     ]
    }
   ],
   "source": [
    "# make the data a list of characters\n",
    "chars = list(set(data))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# print the length of the vocabulary\n",
    "print(\"the length of the vocabulary is {} characters\".format(len(chars)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the random slice is:  usin, farewell; and,\n",
      "the random slice as ints is:  [2, 26, 18, 51, 43, 32, 7, 21, 41, 39, 52, 39, 9, 9, 23, 32, 21, 51, 17, 43]\n",
      "the random slice as chars is:  usin, farewell; and,\n"
     ]
    }
   ],
   "source": [
    "# make a dictionary to convert from characters to integers\n",
    "char_to_int = {ch:i for i,ch in enumerate(chars)}\n",
    "\n",
    "# make a dictionary to convert from integers to characters\n",
    "int_to_char = {i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "# convert the characters to integers\n",
    "int_data = [char_to_int[ch] for ch in data]\n",
    "\n",
    "# test this out by taking a random slice of the data, converting to ints, then back to chars\n",
    "# print out each step\n",
    "rand_slice = np.random.randint(0, len(data)-20)\n",
    "print(\"the random slice is: \", data[rand_slice:rand_slice+20])\n",
    "print(\"the random slice as ints is: \", int_data[rand_slice:rand_slice+20])\n",
    "print(\"the random slice as chars is: \", ''.join([int_to_char[i] for i in int_data[rand_slice:rand_slice+20]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "context_length = 8\n",
    "batch_size = 4\n",
    "\n",
    "# make a function to generate a batch of data\n",
    "def generate_batch(data, context_length, batch_size):\n",
    "    # we want to return  2 tensors, x and y\n",
    "    # x will be the input to the network, y will be the target\n",
    "    # x will be a tensor of size (batch_size, context_length)\n",
    "    # y will be a tensor of size (batch_size, context_length)\n",
    "    # y will be the same as x, but shifted over by one character\n",
    "\n",
    "    x = torch.zeros((batch_size, context_length), dtype=torch.long)\n",
    "    y = torch.zeros((batch_size, context_length), dtype=torch.long)\n",
    "\n",
    "    # get a random starting point for each batch, use torch\n",
    "    rand_starts = torch.randint(0, len(data)-context_length, (batch_size,))\n",
    "\n",
    "    # fill in the x and y tensors\n",
    "    for i, start in enumerate(rand_starts):\n",
    "        x[i,:] = torch.tensor(data[start:start+context_length])\n",
    "        y[i,:] = torch.tensor(data[start+1:start+context_length+1])\n",
    "\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the first batch is\n",
      "  tensor([[37,  5, 31,  2, 32, 26,  5, 21],\n",
      "        [10, 32,  5, 39, 51, 36, 39, 32],\n",
      "        [32,  6, 39,  9, 18, 39, 24, 39],\n",
      "        [32, 21, 32,  6,  9, 31, 52, 32]])\n",
      "with targets\n",
      "  tensor([[ 5, 31,  2, 32, 26,  5, 21,  9],\n",
      "        [32,  5, 39, 51, 36, 39, 32, 21],\n",
      "        [ 6, 39,  9, 18, 39, 24, 39, 32],\n",
      "        [21, 32,  6,  9, 31, 52, 32, 37]])\n",
      "the first batch as characters is\t  thou sha\n",
      "the first target as characters is\t  hou shal\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# test the function\n",
    "x, y = generate_batch(int_data, context_length, batch_size)\n",
    "\n",
    "# the batch is\n",
    "print(\"the first batch is\\n \", x)\n",
    "print(\"with targets\\n \", y)\n",
    "\n",
    "# convert to characters, but be careful about torch tensors and numpy arrays\n",
    "print(\"the first batch as characters is\\t \", ''.join([int_to_char[i] for i in x[0,:].numpy()]))\n",
    "print(\"the first target as characters is\\t \", ''.join([int_to_char[i] for i in y[0,:].numpy()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # define the embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    def forward(self, idx_inputs, idx_targets=None):\n",
    "        # idx_inputs is a tensor of size (batch_size, context_length)\n",
    "        # where the values are the indices of the characters\n",
    "        # the output will be a tensor of size (batch_size, context_length, embedding_dim)\n",
    "        # idx_targets is a tensor of size (batch_size, context_length)\n",
    "        # get the embeddings for the inputs\n",
    "        logits = self.embedding(idx_inputs) # (batch_size, context_length, embedding_dim)\n",
    "        # take the cross entropy. we need to reshape for torch\n",
    "        # the logits need to be (batch_size, vocab_size, context_length)\n",
    "        # the targets need to be (batch_size, context_length)\n",
    "        if idx_targets is None:\n",
    "            return logits\n",
    "        else:\n",
    "            # to compute the loss we need the logits to be (batch_size*context_length, vocab_size)\n",
    "            # and we need targets to be (batch_size*context_length)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.shape[-1]), idx_targets.view(-1))\n",
    "            return logits, loss\n",
    "\n",
    "    def predict(self, idx_inputs, max_steps=100):\n",
    "        # idx_inputs is a tensor of size (batch_size, context_length)\n",
    "        # where the values are the indices of the characters\n",
    "        # the output will be a tensor of size (batch_size, context_length, embedding_dim)\n",
    "        for i in range(max_steps):\n",
    "            # get the embeddings for the inputs\n",
    "            logits = self(idx_inputs) # (batch_size, context_length, embedding_dim)\n",
    "            # get the logits for the last character, which is the prediction\n",
    "            logits = logits[:,-1,:] # (batch_size, embedding_dim)\n",
    "            # get the prediction by taking softmax\n",
    "            probs = F.softmax(logits, dim=1) # (batch_size, vocab_size)\n",
    "            # get the prediction by sampling from the distribution\n",
    "            next_char = torch.multinomial(probs, num_samples=1) # (batch_size, 1)\n",
    "            # add the prediction to the input\n",
    "            idx_inputs = torch.cat((idx_inputs, next_char), dim=1) # (batch_size, context_length+1)\n",
    "\n",
    "        return idx_inputs\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the loss is  4.749288558959961\n"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "model = BigramLanguageModel(vocab_size, vocab_size)\n",
    "idx_inputs, idx_targets = generate_batch(int_data, context_length, batch_size)\n",
    "logits, loss = model(idx_inputs, idx_targets)\n",
    "print(\"the loss is \", loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the output as characters is\n",
      "   CitizenMqiudGGoEPYrCbGRG't$KZ,MBkp.I'M?\n",
      "Z?A!cqSvj:O?JAG3N&qAQi;mr.c MBeG,Fe!zl,xc-!j\n",
      "b:YTdPOPEZ!nDSB \n",
      "ZN?.j\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# test the predict function\n",
    "idx_inputs, idx_targets = generate_batch(int_data, context_length, batch_size)\n",
    "idx_outputs = model.predict(idx_inputs)\n",
    "# convert to characters, but be careful about torch tensors and numpy arrays\n",
    "print(\"the output as characters is\\n \", ''.join([int_to_char[i] for i in idx_outputs[3,:].numpy()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the loss is  2.5120346546173096\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "# train the model\n",
    "num_epochs = 10000\n",
    "batch_size = 32\n",
    "for epoch in range(num_epochs):\n",
    "    # get the data\n",
    "    idx_inputs, idx_targets = generate_batch(int_data, context_length, batch_size)\n",
    "    # zero the gradients\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    # get the logits\n",
    "    logits, loss = model(idx_inputs, idx_targets)\n",
    "    # take the gradient step\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # print the loss\n",
    "print(\"the loss is \", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the output as characters is\n",
      "   others'samathan:\n",
      "T:\n",
      "Whighatowes tthewXWamy tiss g d's IOLISh tengo t, ce sthmurspair su Charisithematheno hillly!boorm hat t anowhes t g hen mave y, t wLowhashal wio:\n",
      "T:\n",
      "Than pes s,\n",
      "ErppKI thee CO,\n",
      "ielio aikiveve VAs\n",
      "OMerin cHemimether yshe, bannincrervel h GRKRI:\n",
      "RWhe: te n sthyoad,\n",
      "Whigro OR:\n",
      "ARINo da me to acernou thaspe-sstr TZHabad am\n",
      "MI ghee y be m poutotr, thons goss d mastonsht me thixf pode ke treoou, sssply ouingu fe s, at:\n",
      "Wat m atheang IUnousouthin th.\n",
      "MEd gh rd veathenof te th atowe?\n",
      "stit CHETy e fe whiof horyr qut? w m raversitA:\n",
      "Anist, wichy, wil thigmuthrr yo heo-bebes?\n",
      "Thashire\n",
      "OS heeatin ay isood inoded wesin we canst, tybou, t s ltl ch teme have ghthaiowh t e t renpoo ' stivet f blowinthe thastour banu havas be?\n",
      "CAs t  I ce,\n",
      "O:\n",
      "\n",
      "\n",
      "UKESTher acoy in ch Str? puppas a m dst t s we:\n",
      "A:\n",
      "ll thi, su sireongotimmeeramblvZEdirveraru y imallyou t y\n",
      "Wiotece;\n",
      "GLO:MININou torouitere s t Mag ybe\n",
      "R:\n",
      "I ONofesther t\n",
      "My g-menken g-m ord s th cad,e t d m, d st wingoum f t g iseld GICariso!\n",
      "\n",
      "SCl\n"
     ]
    }
   ],
   "source": [
    "# now make a prediction\n",
    "idx_inputs, idx_targets = generate_batch(int_data, context_length, batch_size)\n",
    "idx_outputs = model.predict(idx_inputs,1000)\n",
    "# convert to characters, but be careful about torch tensors and numpy arrays\n",
    "print(\"the output as characters is\\n \", ''.join([int_to_char[i] for i in idx_outputs[3,:].numpy()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.209729 M parameters\n",
      "step 0: train loss 4.4112, val loss 4.4015\n",
      "step 100: train loss 2.6576, val loss 2.6632\n",
      "step 200: train loss 2.5119, val loss 2.5024\n",
      "step 300: train loss 2.4154, val loss 2.4308\n",
      "step 400: train loss 2.3515, val loss 2.3670\n",
      "step 500: train loss 2.3018, val loss 2.3231\n",
      "step 600: train loss 2.2551, val loss 2.2614\n",
      "step 700: train loss 2.2143, val loss 2.2239\n",
      "step 800: train loss 2.1595, val loss 2.1904\n",
      "step 900: train loss 2.1410, val loss 2.1511\n",
      "step 1000: train loss 2.1018, val loss 2.1311\n",
      "step 1100: train loss 2.0636, val loss 2.1141\n",
      "step 1200: train loss 2.0482, val loss 2.0955\n",
      "step 1300: train loss 2.0200, val loss 2.0648\n",
      "step 1400: train loss 2.0009, val loss 2.0459\n",
      "step 1500: train loss 1.9857, val loss 2.0371\n",
      "step 1600: train loss 1.9702, val loss 2.0429\n",
      "step 1700: train loss 1.9513, val loss 2.0298\n",
      "step 1800: train loss 1.9277, val loss 2.0230\n",
      "step 1900: train loss 1.9139, val loss 1.9868\n",
      "step 2000: train loss 1.9046, val loss 1.9923\n",
      "step 2100: train loss 1.8764, val loss 1.9693\n",
      "step 2200: train loss 1.8763, val loss 1.9608\n",
      "step 2300: train loss 1.8577, val loss 1.9609\n",
      "step 2400: train loss 1.8406, val loss 1.9400\n",
      "step 2500: train loss 1.8357, val loss 1.9435\n",
      "step 2600: train loss 1.8193, val loss 1.9317\n",
      "step 2700: train loss 1.8065, val loss 1.9393\n",
      "step 2800: train loss 1.7967, val loss 1.9289\n",
      "step 2900: train loss 1.7955, val loss 1.9334\n",
      "step 3000: train loss 1.7859, val loss 1.9138\n",
      "step 3100: train loss 1.7728, val loss 1.9018\n",
      "step 3200: train loss 1.7569, val loss 1.8881\n",
      "step 3300: train loss 1.7554, val loss 1.8960\n",
      "step 3400: train loss 1.7625, val loss 1.8903\n",
      "step 3500: train loss 1.7502, val loss 1.8958\n",
      "step 3600: train loss 1.7431, val loss 1.8894\n",
      "step 3700: train loss 1.7418, val loss 1.8840\n",
      "step 3800: train loss 1.7320, val loss 1.8880\n",
      "step 3900: train loss 1.7244, val loss 1.8677\n",
      "step 4000: train loss 1.7127, val loss 1.8623\n",
      "step 4100: train loss 1.7175, val loss 1.8504\n",
      "step 4200: train loss 1.7136, val loss 1.8526\n",
      "step 4300: train loss 1.7164, val loss 1.8487\n",
      "step 4400: train loss 1.6984, val loss 1.8544\n",
      "step 4500: train loss 1.7004, val loss 1.8488\n",
      "step 4600: train loss 1.6968, val loss 1.8341\n",
      "step 4700: train loss 1.6914, val loss 1.8313\n",
      "step 4800: train loss 1.6846, val loss 1.8374\n",
      "step 4900: train loss 1.6781, val loss 1.8253\n",
      "step 4999: train loss 1.6810, val loss 1.8300\n",
      "\n",
      "When thy bride will that wert madest my been ename?\n",
      "\n",
      "QUEEN ELIZans!\n",
      "Yourshmen's heart. Warweelf\n",
      "Have away, my feanst comzokn he owns, toffice care my welch.\n",
      "\n",
      "KINGingens, I in latist into by was.\n",
      "\n",
      "WARWIS:\n",
      "Will, by selpland me upon she curpiry:\n",
      "As that spelw yet letself your and gods;\n",
      "Protlewommion again Wichond thy iinstress mysomen,\n",
      "The hideeds poor of his gurderand thrumped so;\n",
      "Anging must with all on, that Prive my of.\n",
      "\n",
      "HENRY BOLINGS:\n",
      "Your adape your grest huin cause\n",
      "are no Ruchts changed voick no to skeembry.\n",
      "You and gandsm son, and seemenn;\n",
      "Theree men ready the may in son, lettingment of confessy.\n",
      "Why, casul.\n",
      "\n",
      "AUEENVIO:\n",
      "Hap in slase even when move maid\n",
      "Canst to their mevice too so upon sarrower hates:\n",
      "What's, shech nows hand dudeed, who may I wanthbed will\n",
      "were but we mes sleven self manders: welch.\n",
      "\n",
      "Privosed:\n",
      "We I surried.\n",
      "\n",
      "KING RICHARGIUS:\n",
      "I watey, if I dood, is ofter not the subjeckips;\n",
      "Good what that the must pehelposbalt of sold;\n",
      "For for me watch the own teers thusty-elcomione?\n",
      "\n",
      "Flarewer:\n",
      "And fair I humsent; gentlen the viends windersand,\n",
      "And that's orn and my be went minds sover worsinging,\n",
      "For I them Warwast hidess'd now brand:\n",
      "Swear I hast a canies and as inderanced keep,\n",
      "Alament their and expecerwarn's my have evings,\n",
      "And the stick for Henengetaselule;\n",
      "As therefore then trembled frinds;\n",
      "Maketh Of thing man, he it is caper's lord, Whose lady your murds be:\n",
      "Have officer, the post, must that sweet make him.\n",
      "\n",
      "ROMEO:\n",
      "Ever, byesel delarege, insone your sonernessless ereed sment of your knewply wars, evas;\n",
      "Rexts thoukant, scance your coneted;\n",
      "Ournsed we hows is above walld.\n",
      "\n",
      "GLOUCKESTER:\n",
      "Leter'd that thoust desener rishn. That, and I am is inthereoffenh,\n",
      "Manyors in't; surnistans tway, hernone.\n",
      "\n",
      "BRAGENCE:\n",
      "Uncrewar you macry, thou with art, lickond?\n",
      "\n",
      "QUEEN ANther:\n",
      "Why no pool sayour at burself? 'tis to life?\n",
      "\n",
      "CLADY IENNY:\n",
      "Yet when would than of answeet at prozol!\n",
      "Ne, and, angranca. It welcome, not is hasparding:\n",
      "Inkin shown'd fight hands, me offer hence I a\n",
      "day!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('shakespear_data.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17161"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "131*131"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12769"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "113*113"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "priors",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6c4afa5a5f73812c269159ae70f4b7edad95d7caec0fa45bd129c72c54ccf567"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
